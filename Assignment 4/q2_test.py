# -*- coding: utf-8 -*-
"""Q2_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R-h6mR7sy_h7Yi1dKE4xCb8nKprgkVDj
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from linearRegression.linear_regression import LinearRegression
from metrics import *

np.random.seed(45)

N = 90
P = 10
X = pd.DataFrame(np.random.randn(N, P))
y = pd.Series(np.random.randn(N))
print(X.shape)


print('Batch Gradient Descent with manual gradient computation for unregularized objective : ')


LR = LinearRegression()
# Call Gradient Descent here
LR.fit_gradient_descent(X,y,batch_size=10,gradient_type='jax',penalty_type='none')
y_hat = LR.predict(X)

print(' Batch size=',10,', RMSE: ', rmse(pd.Series(y_hat), y))
print(' Batch size=',10,', MAE: ', mae(pd.Series(y_hat), y))
  
print("---------------------------")



#TODO :  Call the different variants of gradient descent here (as given in Q2)
LR.fit_gradient_descent(X,y,batch_size=10,gradient_type='jax',penalty_type='l1')
y_hat = LR.predict(X)

print(' Batch size=',10,', RMSE: ', rmse(pd.Series(y_hat), y))
print(' Batch size=',10,', MAE: ', mae(pd.Series(y_hat), y))
  
print("---------------------------")
LR.fit_gradient_descent(X,y,batch_size=10,gradient_type='jax',penalty_type='l2')
y_hat = LR.predict(X)

print(' Batch size=',10,', RMSE: ', rmse(pd.Series(y_hat), y))
print(' Batch size=',10,', MAE: ', mae(pd.Series(y_hat), y))
  
print("---------------------------")
LR.fit_gradient_descent(X,y,batch_size=10,gradient_type='manual',penalty_type='none')
y_hat = LR.predict(X)

print(' Batch size=',10,', RMSE: ', rmse(pd.Series(y_hat), y))
print(' Batch size=',10,', MAE: ', mae(pd.Series(y_hat), y))
  
print("---------------------------")
LR.fit_gradient_descent(X,y,batch_size=10,gradient_type='manual',penalty_type='l2')
y_hat = LR.predict(X)

print(' Batch size=',10,', RMSE: ', rmse(pd.Series(y_hat), y))
print(' Batch size=',10,', MAE: ', mae(pd.Series(y_hat), y))
  
print("---------------------------")